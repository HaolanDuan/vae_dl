{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0866333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] 找不到指定的程序。\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d53d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=10\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645729d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a03d40ea4d94cc099b3b47644a0c362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970629c2f12642e5979112e201caa885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e9e34f9e42406ea5513e7353a05a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b80bfb96b446c49be7240b2fb21d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.MNIST(\n",
    "    root='MNIST',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_data=datasets.MNIST(\n",
    "    root='MNIST',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119c5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd02a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_data,batch_size=128,shuffle=True)\n",
    "test_loader=DataLoader(dataset=test_data,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eadf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = Variable(std.data.new(std.size()).normal_())\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "model = VAE()\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f592c1",
   "metadata": {},
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x.view(-1, 784))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "   KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "   return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd56c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    :param recon_x: generated image\n",
    "    :param x: original image\n",
    "    :param mu: latent mean of z\n",
    "    :param logvar: latent log variance of z\n",
    "    \"\"\"\n",
    "    BCE_loss = nn.BCELoss(reduction='sum')\n",
    "    reconstruction_loss = BCE_loss(recon_x, x)\n",
    "    KL_divergence = -0.5 * torch.sum(1+logvar-torch.exp(logvar)-mu**2)\n",
    "    #KLD_ele = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    #KLD = torch.sum(KLD_ele).mul_(-0.5)\n",
    "    #print(reconstruction_loss, KL_divergence)\n",
    "\n",
    "    return reconstruction_loss + KL_divergence\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848324b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = torch.flatten(data, start_dim=1)\n",
    "            \n",
    "        data = Variable(data)\n",
    "        data = data.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        #train_loss += loss.data[0].item()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()/ len(data)))\n",
    "            \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e836bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = torch.flatten(data, start_dim=1)\n",
    "        \n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1b57a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 548.946899\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 466.735321\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 364.735596\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 296.344666\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 260.867554\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 248.070862\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 238.226730\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 230.579041\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 230.717865\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 217.459991\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 228.901001\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 215.922485\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 218.032745\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 213.523361\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 226.292160\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 205.217712\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 198.604294\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 201.819672\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 191.741013\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 199.797867\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 186.238113\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 192.329163\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 189.704422\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 180.651306\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 180.611404\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 183.775620\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 177.574524\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 178.574066\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 175.830551\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 173.042923\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 173.655655\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 166.355270\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 161.039368\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 165.939148\n",
      "Train Epoch: 0 [43520/60000 (72%)]\tLoss: 167.132797\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 167.515808\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 164.053146\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 165.492386\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 162.635147\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 159.398483\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 162.517029\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 155.400482\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 153.224869\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 156.697632\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 158.573700\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 154.189453\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 153.380890\n",
      "====> Epoch: 0 Average loss: 204.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_18616\\194371317.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data = Variable(data, volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 154.8200\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 149.634262\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 149.229431\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 157.230637\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 150.369995\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 152.494202\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 154.351486\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 152.738800\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 154.166565\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 152.219391\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 151.797516\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 144.826660\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 149.520264\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 146.518173\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 149.737183\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 141.571686\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 146.652252\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 146.878891\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 148.367035\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 145.134918\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 148.326416\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 142.567993\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 147.534012\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 140.332916\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 141.713715\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 145.754364\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 140.887680\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 146.188507\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 142.998383\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 145.444427\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 140.434723\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 134.828293\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 140.960114\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 143.611313\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 132.660385\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 134.250305\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 138.154114\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 134.315399\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 135.402344\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 133.728622\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 137.394394\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 138.039978\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 133.038116\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 140.227371\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 134.778351\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 130.945465\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 132.166061\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 135.561905\n",
      "====> Epoch: 1 Average loss: 142.9837\n",
      "====> Test set loss: 132.5554\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 130.475494\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 131.318253\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 131.405304\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 129.950516\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 129.958832\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 132.921722\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 134.072739\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 129.928009\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 132.756546\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 131.469589\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 130.709625\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 132.798477\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 127.363037\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 124.735756\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 133.842514\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 132.097046\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 129.129745\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 132.135681\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 125.754486\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 131.314758\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 127.266373\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 125.214668\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 127.967300\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 130.209900\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 123.937485\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 127.735954\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 127.314262\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 130.643845\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 130.367752\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 131.397156\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 125.232758\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 128.804779\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 126.083771\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 129.653793\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 122.449745\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 126.316437\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 127.002510\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 127.841644\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 125.096054\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 123.711189\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 126.473816\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 119.841545\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 121.925735\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 122.269440\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 122.840683\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 124.462624\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 126.334267\n",
      "====> Epoch: 2 Average loss: 128.4804\n",
      "====> Test set loss: 123.0953\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 126.646896\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 125.961044\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 123.351730\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 130.359512\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 127.628746\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 127.251648\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 126.335663\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 124.144501\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 119.142410\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 124.137619\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 128.470947\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 125.287872\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 117.706093\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 125.741272\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 121.560638\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 124.987724\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 124.209229\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 125.772469\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 121.337952\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 126.688438\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 122.646118\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 121.594543\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 121.233810\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 123.203583\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 119.853798\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 119.621841\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 120.083755\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 123.585571\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 122.492577\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 124.502548\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 122.432304\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 121.662537\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 120.572296\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 120.425903\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 119.395844\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 121.632706\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 121.865761\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 120.852486\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 117.761948\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 123.876457\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 112.512016\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 114.966782\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 122.381363\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 123.775490\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 125.043732\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 121.112076\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 122.355820\n",
      "====> Epoch: 3 Average loss: 121.7812\n",
      "====> Test set loss: 118.4486\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 122.309380\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 122.760483\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 121.780205\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 121.108551\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 117.799286\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 123.620789\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 119.850922\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 117.239395\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 120.643623\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 121.447029\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 121.319733\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 121.131882\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 121.182297\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 118.242157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 119.053131\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 119.009018\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 115.246140\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 118.115860\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 118.726387\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 118.266266\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 118.517426\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 118.973274\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 118.912918\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 116.068115\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 116.901176\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 116.344131\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 119.905640\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 117.541397\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 121.034149\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 115.156097\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 118.291443\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 119.244949\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 113.715683\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 115.723434\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 112.566528\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 114.638168\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 114.972458\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 117.303238\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 114.154251\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 114.776001\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 114.243492\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 116.963036\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 119.216850\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 116.155136\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 114.092926\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 114.174042\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 118.984764\n",
      "====> Epoch: 4 Average loss: 117.9839\n",
      "====> Test set loss: 115.3745\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 115.307037\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 113.971558\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 116.381378\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 115.830338\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 119.380028\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 117.956436\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 117.771973\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 111.912285\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 108.547234\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 111.881149\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 114.566879\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 111.323730\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 114.914017\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 114.972076\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 114.864250\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 115.865402\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 114.414833\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 121.051132\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 112.493088\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 115.276245\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 115.224945\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 112.349045\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 111.690880\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 114.671265\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 113.119820\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 115.200523\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 116.982002\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 113.484093\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 112.278976\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 109.993347\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 113.792900\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 117.913895\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 116.035866\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 114.687225\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 116.126534\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 118.446503\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 115.414322\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 112.690170\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 117.412193\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 114.633339\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 115.577316\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 117.832794\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 113.828964\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 117.466469\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 119.384232\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 114.375710\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 110.575699\n",
      "====> Epoch: 5 Average loss: 115.4979\n",
      "====> Test set loss: 113.5042\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 115.841164\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 108.802505\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 111.512421\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 119.362915\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 110.690414\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 111.627838\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 114.670715\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 111.554047\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 112.660034\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 108.273788\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 112.250717\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 116.240410\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 114.062508\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 110.302765\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 112.963394\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 112.977829\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 111.194046\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 112.301720\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 115.342316\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 110.611221\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 112.241859\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 118.153152\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 111.242691\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 112.702003\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 116.997635\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 115.559677\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 114.043777\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 114.237869\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 115.596069\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 114.011711\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 111.721031\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 114.015419\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 111.402512\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 117.347099\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 113.421982\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 110.604828\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 113.872177\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 112.994011\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 112.881332\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 117.806946\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 114.037704\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 112.990974\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 116.733253\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 113.267624\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 111.291687\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 110.271225\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 117.168518\n",
      "====> Epoch: 6 Average loss: 113.6786\n",
      "====> Test set loss: 112.1245\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 111.919556\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 115.686974\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 115.783081\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 111.040451\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 112.732346\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 113.360199\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 111.657707\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 111.935371\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 111.531952\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 115.734909\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 110.846176\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 115.390953\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 110.741096\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 114.930542\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 115.654404\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 110.643028\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 114.281487\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 110.359741\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 114.371063\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 115.831955\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 108.506042\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 114.407265\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 114.790421\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 112.884651\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 115.208290\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 114.821777\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 112.314072\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 110.282715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 111.570435\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 111.188828\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 111.482811\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 110.530075\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 113.365173\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 115.057381\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 110.648338\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 113.876434\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 106.159286\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 110.540237\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 109.620789\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 112.013046\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 113.622948\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 112.331711\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 110.758347\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 111.708351\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 111.541451\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 108.702370\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 112.027885\n",
      "====> Epoch: 7 Average loss: 112.3324\n",
      "====> Test set loss: 110.9377\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 111.524979\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 111.961868\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 112.408493\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 109.272644\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 110.756111\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 109.486931\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 114.627090\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 115.771500\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 110.921051\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 112.362564\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 109.344566\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 112.588623\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 112.976868\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 115.033417\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 116.050598\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 114.504532\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 106.900063\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 112.698441\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 115.492874\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 112.365158\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 108.909065\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 110.727829\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 114.092018\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 112.755676\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 112.690994\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 111.116104\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 107.966347\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 111.675514\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 108.566765\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 116.601822\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.060371\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 110.870598\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 111.324684\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 112.505402\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 112.118927\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 110.731766\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 116.850754\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 108.901703\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 113.363045\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 111.822098\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 118.554474\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 112.740082\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 107.638680\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 110.781174\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 112.040558\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 105.509621\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 108.505386\n",
      "====> Epoch: 8 Average loss: 111.2674\n",
      "====> Test set loss: 109.9579\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 106.272957\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 110.466866\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 112.910751\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 106.789795\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 107.626236\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 108.116249\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 112.593094\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 112.283112\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 113.146606\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 112.455734\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 107.131714\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 111.089737\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 111.330391\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 112.993774\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 111.174500\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 110.664886\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 108.850403\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 111.309128\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 112.166916\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 107.577301\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 109.805916\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 112.383934\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 110.540779\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 112.024414\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 109.736603\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 111.312805\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 113.894264\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 110.330643\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 109.283600\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 112.671082\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 110.202477\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 110.916618\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 108.937439\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 107.642090\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 108.072708\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 110.248650\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 112.343147\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 113.207031\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 113.246735\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 112.196625\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 109.217110\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 109.308121\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 113.257355\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 110.207695\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 108.689941\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 112.154221\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 111.909332\n",
      "====> Epoch: 9 Average loss: 110.3802\n",
      "====> Test set loss: 109.2473\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 104.755768\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 108.328262\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 111.031029\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 107.828041\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 115.052216\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 112.337097\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 109.172974\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 110.714325\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 106.766235\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 112.540466\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 106.075897\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 111.214279\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 109.844650\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 116.389389\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 115.194801\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 109.025177\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 106.266342\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 107.342484\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 110.983925\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 111.161285\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 110.023438\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 108.143379\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 110.334991\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 111.195053\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 105.323242\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 108.736496\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 111.129036\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 109.953117\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 107.462830\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 110.481873\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 111.510033\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 111.111343\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 110.971680\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 113.262512\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 112.328400\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 109.927200\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 111.450607\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 107.250778\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 110.768204\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 109.930832\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 106.640030\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 111.083611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 109.037323\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 111.282860\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 105.860596\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 110.561340\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 105.950768\n",
      "====> Epoch: 10 Average loss: 109.6459\n",
      "====> Test set loss: 108.6038\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 107.055893\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 112.553185\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 108.192535\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 112.933594\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 109.776733\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 111.708069\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 110.801369\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 108.445435\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 111.365654\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 104.486313\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 105.166649\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 107.683769\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 111.095795\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 110.426537\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 108.656158\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 107.966133\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 111.333977\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 103.777863\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 113.533745\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 108.290512\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 108.114182\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 104.786972\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 110.128090\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 105.342278\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 109.551216\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 106.834457\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 108.041031\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 114.361359\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 112.789993\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 110.600128\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 111.764641\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 112.499741\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 105.413979\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 109.720413\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 109.968407\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 109.957771\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 109.300812\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 110.168732\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 109.879372\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 107.599854\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 108.342636\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 107.409454\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 108.141685\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 107.143661\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 107.596481\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 111.140533\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 111.420731\n",
      "====> Epoch: 11 Average loss: 108.9986\n",
      "====> Test set loss: 108.0449\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 107.982590\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 110.362000\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 110.349686\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 105.265015\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 108.231384\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 110.004913\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 106.117294\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 111.250343\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 107.152695\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 109.142326\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 108.825867\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 107.291061\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 107.643364\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 109.958809\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 108.961403\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 109.597214\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 104.519958\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 106.046982\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 107.130692\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 104.834984\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 110.010925\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 109.629532\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 105.694733\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 109.639740\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 108.897598\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 105.739685\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 105.280548\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 107.141098\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 112.393204\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 109.571701\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 112.265488\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 104.512970\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 109.629608\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 107.086670\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 102.865692\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 105.811523\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 109.389130\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 107.424156\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 105.473480\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 106.866859\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 111.419846\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 106.480690\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 106.395691\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 107.854019\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 110.502457\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 108.821014\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 106.692223\n",
      "====> Epoch: 12 Average loss: 108.4776\n",
      "====> Test set loss: 107.5374\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 107.647644\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 109.565628\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 109.968674\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 109.830017\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 109.408997\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 103.946381\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 112.790947\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 108.244125\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 105.606789\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 106.310875\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 105.767822\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 110.838181\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 108.952942\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 110.507431\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 110.685783\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 106.694000\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 110.108276\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 105.457916\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 107.349419\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 105.389015\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 106.107147\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 107.305977\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 110.249176\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 104.545143\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 107.678383\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 109.728714\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 106.967773\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 105.340721\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 106.594170\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 109.262314\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 104.316422\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 109.330078\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 109.257965\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 110.587387\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 105.354263\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 105.289406\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 104.263702\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 109.461380\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 108.444458\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 108.138611\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 104.494385\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 109.110016\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 109.822281\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 110.216026\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 111.514435\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 104.041725\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 110.755127\n",
      "====> Epoch: 13 Average loss: 108.0029\n",
      "====> Test set loss: 107.3242\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 108.315018\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 109.841148\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 107.027405\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 106.210098\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 106.690399\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 105.363312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 108.701416\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 106.955315\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 110.574127\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 108.540962\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 106.387497\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 105.256485\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 109.477310\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 110.595947\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 109.157890\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 106.997871\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 104.277710\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 108.278305\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 108.501129\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 104.084000\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 106.316444\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 105.395828\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 107.137100\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 105.985741\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 107.577103\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 103.080765\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 104.422287\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 108.131409\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 107.115120\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 105.430367\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 106.188240\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 109.238243\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 104.533859\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 109.106743\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 110.657043\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 109.064247\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 107.449005\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 110.927460\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 103.137619\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 103.440979\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 107.113907\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 106.020752\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 106.276253\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 105.662659\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 108.232109\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 108.275787\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 103.846085\n",
      "====> Epoch: 14 Average loss: 107.5796\n",
      "====> Test set loss: 106.8182\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 108.948349\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 110.967300\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 104.513840\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 107.607918\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 109.228065\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 103.838966\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 104.453171\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 109.624352\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 107.534897\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 104.056580\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 107.653419\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 103.566216\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 108.773148\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 109.006073\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 108.516159\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 105.886955\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 106.151154\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 104.547241\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 102.157516\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 108.783501\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 103.171417\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 106.678406\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 107.544998\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 103.981506\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 107.073685\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 104.784081\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 103.080353\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 108.907745\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 109.373085\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 105.570900\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 106.835587\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 107.887306\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 109.661865\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 111.627861\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 110.758354\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 106.208931\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 109.006165\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 106.468117\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 107.977287\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 105.584183\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 110.040451\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 106.933418\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 107.983681\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 105.235596\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 105.754333\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 105.096550\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 105.373466\n",
      "====> Epoch: 15 Average loss: 107.1951\n",
      "====> Test set loss: 106.5497\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 110.649048\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tLoss: 108.703560\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 109.161507\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tLoss: 105.928780\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 108.518669\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 108.345695\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 107.676598\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tLoss: 108.198990\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 106.256744\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tLoss: 109.382637\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 105.492935\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tLoss: 109.460846\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 113.762703\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tLoss: 107.194321\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 104.842438\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 111.294693\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 103.235092\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tLoss: 104.080246\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 110.703400\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tLoss: 109.436066\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 107.531227\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tLoss: 107.938492\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 109.736023\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tLoss: 106.741112\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 109.408234\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 103.257751\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 104.898964\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tLoss: 107.682838\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 107.134659\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tLoss: 107.157700\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 107.471298\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tLoss: 106.194290\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 106.419312\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tLoss: 104.978767\n",
      "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 107.590813\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 110.347328\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 108.269722\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tLoss: 105.850494\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 106.786201\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tLoss: 109.454109\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 106.321083\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tLoss: 104.498955\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 107.232773\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tLoss: 106.022041\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 108.026924\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 108.160500\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 105.988403\n",
      "====> Epoch: 16 Average loss: 106.8723\n",
      "====> Test set loss: 106.2721\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 111.087509\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tLoss: 106.719566\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 109.502182\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tLoss: 102.689255\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 107.989388\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 105.510529\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 103.732574\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tLoss: 104.399513\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 111.632492\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tLoss: 108.832855\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 104.611038\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tLoss: 108.588402\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 111.790359\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tLoss: 109.849930\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 105.304886\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 106.537102\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 104.510300\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tLoss: 107.788918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 105.641975\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tLoss: 109.473541\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 107.398521\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tLoss: 100.247086\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 107.804901\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tLoss: 106.193069\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 108.489861\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 110.502121\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 104.932297\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tLoss: 111.624481\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 113.862091\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tLoss: 102.572113\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 105.897552\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tLoss: 105.693611\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 109.033257\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tLoss: 109.050644\n",
      "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 108.116295\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 108.900528\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 105.901337\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tLoss: 106.348869\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 110.001480\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tLoss: 105.058594\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 105.530075\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tLoss: 110.654800\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 104.961823\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tLoss: 104.769081\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 104.355667\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 108.369492\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 108.293442\n",
      "====> Epoch: 17 Average loss: 106.6118\n",
      "====> Test set loss: 105.9611\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 108.106285\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tLoss: 101.170792\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 105.652756\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tLoss: 108.029701\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 105.285576\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 102.148743\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 108.028458\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tLoss: 104.420357\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 108.177048\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tLoss: 105.556770\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 104.512772\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tLoss: 102.899582\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 104.607872\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tLoss: 108.880913\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 106.366173\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 105.158920\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 110.993050\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tLoss: 110.286530\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 106.219749\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tLoss: 105.308578\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 107.953018\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tLoss: 108.041443\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 107.113731\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tLoss: 102.906860\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 108.826538\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 109.397446\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 101.305511\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tLoss: 103.799393\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 110.256111\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tLoss: 105.369179\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 107.078094\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tLoss: 102.536232\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 106.123795\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tLoss: 106.572128\n",
      "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 108.832993\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 105.794548\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 109.435181\n",
      "Train Epoch: 18 [47360/60000 (79%)]\tLoss: 107.245132\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 105.421661\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tLoss: 106.378151\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 107.523392\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tLoss: 107.650681\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 105.721512\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tLoss: 104.367111\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 104.409805\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 103.653244\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 102.498947\n",
      "====> Epoch: 18 Average loss: 106.3711\n",
      "====> Test set loss: 105.6978\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 107.836067\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tLoss: 106.371040\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 100.870712\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tLoss: 102.071693\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 106.559219\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 105.668777\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 106.946243\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tLoss: 103.437325\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 103.891441\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tLoss: 105.104782\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 100.117516\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tLoss: 110.322861\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 105.348022\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tLoss: 104.605042\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 105.574783\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 107.996338\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 105.987747\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tLoss: 107.722321\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 106.414124\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tLoss: 106.391869\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 106.590500\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tLoss: 109.435577\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 105.652443\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tLoss: 106.705040\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 107.923035\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 104.068367\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 104.498108\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tLoss: 104.755371\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 105.451920\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tLoss: 107.246552\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 111.111343\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tLoss: 103.324799\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 112.219551\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tLoss: 104.797287\n",
      "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 102.752548\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 107.257309\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 105.434052\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tLoss: 105.245590\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 101.037590\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tLoss: 106.441658\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 107.801880\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tLoss: 106.920624\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 103.600761\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tLoss: 106.377380\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 109.587273\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 104.850250\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 106.710907\n",
      "====> Epoch: 19 Average loss: 106.0604\n",
      "====> Test set loss: 105.6877\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c829ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a93543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784]) torch.Size([1, 20]) torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(1,28,28)\n",
    "data = data.cuda()\n",
    "recon_batch, mu, logvar = model(data)\n",
    "print(recon_batch.shape,mu.shape,logvar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb8642b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "torch.Size([1, 784]) tensor([[ 0.5432, -1.6502,  0.3370,  0.2152, -2.3163,  0.6593, -1.7756, -0.3910,\n",
      "         -0.1722,  1.0038, -1.3455,  1.1361,  0.7083, -0.2137, -1.3699,  0.0815,\n",
      "         -0.6160, -0.3507, -0.8501, -0.0521]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([[-1.7752, -3.3116, -1.4625, -3.2386, -2.1979, -2.3793, -2.2774, -3.4573,\n",
      "         -2.3108, -3.0393, -2.2108, -2.1012, -1.7160, -1.9114, -2.0997, -2.6841,\n",
      "         -3.7641, -2.4304, -3.1339, -3.5633]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb/ElEQVR4nO3df2xV9f3H8dctPy4ol+s6aO+9UpvKIBphBIHxY4rVSGOXERFNUJet/DGj40ckaMwY2ez8gzozmcmYLhjDMJPJP4pmMLEGWiAMhqQqosM6ilTpXUNlvRXwdm0/3z8I97vLL/lc7u27t30+kpN47z0v7ruHY1893Hs/DTjnnAAAMFBgPQAAYOCihAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGBmsPUA5+rp6dGxY8cUCoUUCASsxwEAeHLOqaOjQ7FYTAUFl77W6XMldOzYMZWUlFiPAQC4Qs3NzRozZswl9+lz/xwXCoWsRwAAZMHlfD/PWQk9//zzKisr07BhwzRlyhTt3LnzsnL8ExwA9A+X8/08JyW0ceNGLVu2TCtXrlRDQ4NuvfVWVVZW6ujRo7l4OgBAngrkYhXt6dOn6+abb9YLL7yQuu/GG2/UvHnzVFNTc8lsIpFQOBzO9kgAgF7W3t6ukSNHXnKfrF8JdXZ2av/+/aqoqEi7v6KiQrt37z5v/2QyqUQikbYBAAaGrJfQ8ePH1d3dreLi4rT7i4uLFY/Hz9u/pqZG4XA4tfHOOAAYOHL2xoRzX5Byzl3wRaoVK1aovb09tTU3N+dqJABAH5P1zwmNGjVKgwYNOu+qp7W19byrI0kKBoMKBoPZHgMAkAeyfiU0dOhQTZkyRbW1tWn319bWatasWdl+OgBAHsvJignLly/Xj3/8Y02dOlUzZ87U2rVrdfToUT3yyCO5eDoAQJ7KSQktWLBAbW1teuqpp9TS0qIJEyZoy5YtKi0tzcXTAQDyVE4+J3Ql+JwQAPQPJp8TAgDgclFCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwMxg6wGAfFdQ4P+zXCAQ8M709PR4Z5xz3hmgN3ElBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwLmKLPy2SB0NGjR2f0XPfdd1+vZK666irvTHd3t3fmjTfe8M5I0tq1a70zJ06cyOi5MLBxJQQAMEMJAQDMZL2EqqurFQgE0rZIJJLtpwEA9AM5eU3opptu0jvvvJO6PWjQoFw8DQAgz+WkhAYPHszVDwDgG+XkNaHGxkbFYjGVlZXp/vvv1+HDhy+6bzKZVCKRSNsAAAND1kto+vTpevnll7V161a9+OKLisfjmjVrltra2i64f01NjcLhcGorKSnJ9kgAgD4q6yVUWVmpe++9VxMnTtSdd96pzZs3S5LWr19/wf1XrFih9vb21Nbc3JztkQAAfVTOP6x69dVXa+LEiWpsbLzg48FgUMFgMNdjAAD6oJx/TiiZTOrjjz9WNBrN9VMBAPJM1kvo8ccfV319vZqamrR3717dd999SiQSqqqqyvZTAQDyXNb/Oe7zzz/XAw88oOPHj2v06NGaMWOG9uzZo9LS0mw/FQAgzwWcc856iP+VSCQUDoetx0COBAIB78zYsWO9M7/97W+9M5I0Z84c70wmi5H2lmQymVHuww8/9M5kspDrkSNHvDPIH+3t7Ro5cuQl92HtOACAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGZYwBS96pprrvHO/P73v/fOzJs3zzsjScOGDfPOdHd3e2e+/vpr78ygQYN6JSNJp0+f9s689dZb3pmf/vSn3plMZoMNFjAFAPRplBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzg60HQP7KZIXm73//+72SyWRla0l67733vDOrVq3yzjQ2NnpnJk+e7J1ZtGiRd0aSrr/+eu/M7NmzvTPl5eXemb/97W/eGfRdXAkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwwwKmyNiwYcO8Mz/60Y+8M6FQyDvT3NzsnZGkRx991Dvz7rvveme6urq8M5999pl3JhgMemck6amnnvLOjBgxwjvzgx/8wDtTW1vrncnkeKN3cCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADAuYImNDhgzxzkQiEe/M4MH+p2lbW5t3RspskdBMFsd0zvXK8xQUZPZzZnd3t3cmk7+nwsJC70ymXxP6Jv42AQBmKCEAgBnvEtqxY4fmzp2rWCymQCCgTZs2pT3unFN1dbVisZiGDx+u8vJyHTx4MFvzAgD6Ee8SOnnypCZNmqQ1a9Zc8PFnnnlGq1ev1po1a7Rv3z5FIhHNmTNHHR0dVzwsAKB/8X4lsbKyUpWVlRd8zDmn5557TitXrtT8+fMlSevXr1dxcbE2bNighx9++MqmBQD0K1l9TaipqUnxeFwVFRWp+4LBoG677Tbt3r37gplkMqlEIpG2AQAGhqyWUDwelyQVFxen3V9cXJx67Fw1NTUKh8OpraSkJJsjAQD6sJy8Oy4QCKTdds6dd99ZK1asUHt7e2prbm7OxUgAgD4oqx9WPftBxHg8rmg0mrq/tbX1vKujs4LBoILBYDbHAADkiaxeCZWVlSkSiai2tjZ1X2dnp+rr6zVr1qxsPhUAoB/wvhL66quv9Omnn6ZuNzU16b333lNhYaGuu+46LVu2TKtWrdK4ceM0btw4rVq1SldddZUefPDBrA4OAMh/3iX07rvv6vbbb0/dXr58uSSpqqpKf/rTn/TEE0/o9OnTWrRokU6cOKHp06fr7bffVigUyt7UAIB+wbuEysvLL7n4YiAQUHV1taqrq69kLuSBnp4e78yXX37pnfnvf//rnclkAU5JGb07M9PFUn3FYjHvzOTJkzN6rqFDh3pnTp8+7Z3ZuXOndybTv1v0TawdBwAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwk9XfrIqB5dSpU96ZvXv3ememTp3qnRk7dqx3RpIeffRR78zmzZu9M5n8apOKigrvTKaraGeyQnpjY6N3pqGhwTtzqVX8kX+4EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGBUyRse7ubu9MbW2td+bee+/1zowfP947I2W2SOidd97pnRkyZEivZLq6urwzkhSPx70zn3zyiXemra3NO8MCpv0LV0IAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMsIApMpbJQpL/+te/vDMNDQ3emdLSUu+MJI0YMcI7k8nCooFAwDvT2dnpnWltbfXOSNLhw4e9M59//rl3pqenxzuTybFj0dO+iyshAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZljAFL3q1KlT3pl//OMf3pmZM2d6Z3pTc3Ozd2br1q3emffff987I0mFhYXemVgs5p2ZPHmyd+aLL77wziSTSe8MegdXQgAAM5QQAMCMdwnt2LFDc+fOVSwWUyAQ0KZNm9IeX7hwoQKBQNo2Y8aMbM0LAOhHvEvo5MmTmjRpktasWXPRfe666y61tLSkti1btlzRkACA/sn7jQmVlZWqrKy85D7BYFCRSCTjoQAAA0NOXhOqq6tTUVGRxo8fr4ceeuiSv2I4mUwqkUikbQCAgSHrJVRZWalXXnlF27Zt07PPPqt9+/bpjjvuuOhbJGtqahQOh1NbSUlJtkcCAPRRWf+c0IIFC1L/PWHCBE2dOlWlpaXavHmz5s+ff97+K1as0PLly1O3E4kERQQAA0TOP6wajUZVWlqqxsbGCz4eDAYVDAZzPQYAoA/K+eeE2tra1NzcrGg0muunAgDkGe8roa+++kqffvpp6nZTU5Pee+89FRYWqrCwUNXV1br33nsVjUZ15MgR/eIXv9CoUaN0zz33ZHVwAED+8y6hd999V7fffnvq9tnXc6qqqvTCCy/owIEDevnll/Wf//xH0WhUt99+uzZu3KhQKJS9qQEA/YJ3CZWXl8s5d9HHM1lkEQPHpc6di+ns7PTOdHV1eWck6fPPP/fObN++3Tuzdu1a70xLS4t3ZtCgQd4ZSRo/frx35jvf+Y53pry83DvT0NDgnTl8+LB3Br2DteMAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGZy/ptVgSuVyYrOmf623v3793tnfve733ln/v3vf3tnMlmBPBAIeGekzFbszsSIESO8MzfeeKN35siRI94ZSerp6ckoh8vHlRAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzLGCKXjV06FDvzLRp03IwyYXt3LnTO3P8+HHvTG8tjJnJoqeS9OWXX3pnPvnkE+9MUVGRd2bChAnemXfeecc7I0nJZDKjHC4fV0IAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMsIApetW3v/1t78wNN9zgnSkoyOznq7a2Nu9Md3d3Rs/Vl2XyNXV1dXlnrr32Wu/MP//5T+9MIBDwzqB3cCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADAuYolfdcccd3plYLOad6ezs9M5IUjQa9c5kslhqT0+PdyYTmS7cec0113hnZs2a5Z351re+5Z05duyYdyaTxVXRO7gSAgCYoYQAAGa8SqimpkbTpk1TKBRSUVGR5s2bp0OHDqXt45xTdXW1YrGYhg8frvLych08eDCrQwMA+gevEqqvr9fixYu1Z88e1dbWqqurSxUVFTp58mRqn2eeeUarV6/WmjVrtG/fPkUiEc2ZM0cdHR1ZHx4AkN+83pjw1ltvpd1et26dioqKtH//fs2ePVvOOT333HNauXKl5s+fL0lav369iouLtWHDBj388MPZmxwAkPeu6DWh9vZ2SVJhYaEkqampSfF4XBUVFal9gsGgbrvtNu3evfuCf0YymVQikUjbAAADQ8Yl5JzT8uXLdcstt2jChAmSpHg8LkkqLi5O27e4uDj12LlqamoUDodTW0lJSaYjAQDyTMYltGTJEn3wwQf6y1/+ct5j5342wTl30c8rrFixQu3t7amtubk505EAAHkmow+rLl26VG+++aZ27NihMWPGpO6PRCKSzlwR/e+H/lpbW8+7OjorGAwqGAxmMgYAIM95XQk557RkyRK99tpr2rZtm8rKytIeLysrUyQSUW1tbeq+zs5O1dfXZ/RpagBA/+Z1JbR48WJt2LBBb7zxhkKhUOp1nnA4rOHDhysQCGjZsmVatWqVxo0bp3HjxmnVqlW66qqr9OCDD+bkCwAA5C+vEnrhhRckSeXl5Wn3r1u3TgsXLpQkPfHEEzp9+rQWLVqkEydOaPr06Xr77bcVCoWyMjAAoP/wKiHn3DfuEwgEVF1drerq6kxnQj+WycKdl3PenWv48OHeGUlasGCBd2br1q3emS+++MI7k4lrr702o9yiRYu8M5WVld6Z06dPe2c++ugj7wwLmPZdrB0HADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADCT0W9WBTK1a9cu78zhw4e9M9dff713RpK++93vemdeeukl78z777/vnYnFYt6ZKVOmeGek//8tyT4yWSH97bff9s58+OGH3hn0XVwJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMBNwzjnrIf5XIpFQOBy2HgM5Mniw/5q5c+bM8c785je/8c5IUllZmXdm+PDh3plAIOCdKSjw/5kxk0VFJamzs9M7s3fvXu/MT37yE+/M0aNHvTOw0d7erpEjR15yH66EAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmGEBU/R5wWDQOzNlypSMnutXv/qVd+bmm2/2zowYMcI7k8miol988YV3RpL++te/emdeeeUV78zBgwe9M93d3d4Z2GABUwBAn0YJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMC5gCBgKBQK9kMv3fu499W0CeYgFTAECfRgkBAMx4lVBNTY2mTZumUCikoqIizZs3T4cOHUrbZ+HChQoEAmnbjBkzsjo0AKB/8Cqh+vp6LV68WHv27FFtba26urpUUVGhkydPpu131113qaWlJbVt2bIlq0MDAPqHwT47v/XWW2m3161bp6KiIu3fv1+zZ89O3R8MBhWJRLIzIQCg37qi14Ta29slSYWFhWn319XVqaioSOPHj9dDDz2k1tbWi/4ZyWRSiUQibQMADAwZv0XbOae7775bJ06c0M6dO1P3b9y4USNGjFBpaamampr0y1/+Ul1dXdq/f7+CweB5f051dbV+/etfZ/4VAHmIt2hjILict2hnXEKLFy/W5s2btWvXLo0ZM+ai+7W0tKi0tFSvvvqq5s+ff97jyWRSyWQydTuRSKikpCSTkYC8QQlhILicEvJ6TeispUuX6s0339SOHTsuWUCSFI1GVVpaqsbGxgs+HgwGL3iFBADo/7xKyDmnpUuX6vXXX1ddXZ3Kysq+MdPW1qbm5mZFo9GMhwQA9E9eb0xYvHix/vznP2vDhg0KhUKKx+OKx+M6ffq0JOmrr77S448/rr///e86cuSI6urqNHfuXI0aNUr33HNPTr4AAEAecx4kXXBbt26dc865U6dOuYqKCjd69Gg3ZMgQd91117mqqip39OjRy36O9vb2iz4PG1t/2QKBgPdWUFDgvWXyPIFAwPz4sPWPrb29/Ru/57OAKWCANyZgIMjZGxMAXJlMvslTDOiPWMAUAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAmT5XQs456xEAAFlwOd/P+1wJdXR0WI8AAMiCy/l+HnB97NKjp6dHx44dUygUUiAQSHsskUiopKREzc3NGjlypNGE9jgOZ3AczuA4nMFxOKMvHAfnnDo6OhSLxVRQcOlrncG9NNNlKygo0JgxYy65z8iRIwf0SXYWx+EMjsMZHIczOA5nWB+HcDh8Wfv1uX+OAwAMHJQQAMBMXpVQMBjUk08+qWAwaD2KKY7DGRyHMzgOZ3Aczsi349Dn3pgAABg48upKCADQv1BCAAAzlBAAwAwlBAAwk1cl9Pzzz6usrEzDhg3TlClTtHPnTuuRelV1dbUCgUDaFolErMfKuR07dmju3LmKxWIKBALatGlT2uPOOVVXVysWi2n48OEqLy/XwYMHbYbNoW86DgsXLjzv/JgxY4bNsDlSU1OjadOmKRQKqaioSPPmzdOhQ4fS9hkI58PlHId8OR/ypoQ2btyoZcuWaeXKlWpoaNCtt96qyspKHT161Hq0XnXTTTeppaUltR04cMB6pJw7efKkJk2apDVr1lzw8WeeeUarV6/WmjVrtG/fPkUiEc2ZM6ffrUP4TcdBku66666082PLli29OGHu1dfXa/HixdqzZ49qa2vV1dWliooKnTx5MrXPQDgfLuc4SHlyPrg88b3vfc898sgjaffdcMMN7uc//7nRRL3vySefdJMmTbIew5Qk9/rrr6du9/T0uEgk4p5++unUfV9//bULh8Puj3/8o8GEvePc4+Ccc1VVVe7uu+82mcdKa2urk+Tq6+udcwP3fDj3ODiXP+dDXlwJdXZ2av/+/aqoqEi7v6KiQrt37zaaykZjY6NisZjKysp0//336/Dhw9YjmWpqalI8Hk87N4LBoG677bYBd25IUl1dnYqKijR+/Hg99NBDam1ttR4pp9rb2yVJhYWFkgbu+XDucTgrH86HvCih48ePq7u7W8XFxWn3FxcXKx6PG03V+6ZPn66XX35ZW7du1Ysvvqh4PK5Zs2apra3NejQzZ//+B/q5IUmVlZV65ZVXtG3bNj377LPat2+f7rjjDiWTSevRcsI5p+XLl+uWW27RhAkTJA3M8+FCx0HKn/Ohz62ifSnn/moH59x59/VnlZWVqf+eOHGiZs6cqbFjx2r9+vVavny54WT2Bvq5IUkLFixI/feECRM0depUlZaWavPmzZo/f77hZLmxZMkSffDBB9q1a9d5jw2k8+FixyFfzoe8uBIaNWqUBg0adN5PMq2tref9xDOQXH311Zo4caIaGxutRzFz9t2BnBvni0ajKi0t7Zfnx9KlS/Xmm29q+/btab/6ZaCdDxc7DhfSV8+HvCihoUOHasqUKaqtrU27v7a2VrNmzTKayl4ymdTHH3+saDRqPYqZsrIyRSKRtHOjs7NT9fX1A/rckKS2tjY1Nzf3q/PDOaclS5botdde07Zt21RWVpb2+EA5H77pOFxInz0fDN8U4eXVV191Q4YMcS+99JL76KOP3LJly9zVV1/tjhw5Yj1ar3nsscdcXV2dO3z4sNuzZ4/74Q9/6EKhUL8/Bh0dHa6hocE1NDQ4SW716tWuoaHBffbZZ845555++mkXDofda6+95g4cOOAeeOABF41GXSKRMJ48uy51HDo6Otxjjz3mdu/e7Zqamtz27dvdzJkz3bXXXtuvjsPPfvYzFw6HXV1dnWtpaUltp06dSu0zEM6HbzoO+XQ+5E0JOefcH/7wB1daWuqGDh3qbr755rS3Iw4ECxYscNFo1A0ZMsTFYjE3f/58d/DgQeuxcm779u1O0nlbVVWVc+7M23KffPJJF4lEXDAYdLNnz3YHDhywHToHLnUcTp065SoqKtzo0aPdkCFD3HXXXeeqqqrc0aNHrcfOqgt9/ZLcunXrUvsMhPPhm45DPp0P/CoHAICZvHhNCADQP1FCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADDzf5eXuIdkCq7tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x,y=next(iter(test_loader))\n",
    "print(x[0].shape)\n",
    "x = x[0]\n",
    "x = x.cuda()\n",
    "recon_batch, mu, logvar = model(x)\n",
    "print(recon_batch.shape,mu,logvar)\n",
    "img=recon_batch.reshape(28,28)\n",
    "img = img.cpu().detach()\n",
    "print(img.shape)\n",
    "img = img.numpy()\n",
    "plt.imshow(img,cmap ='gray')\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "125d38eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = torch.randn(1,20)\n",
    "data = data.cuda()\n",
    "y=model.decode(data)\n",
    "img=y.reshape(28,28)\n",
    "img = img.cpu().detach()\n",
    "print(img.shape)\n",
    "img = img.numpy()\n",
    "plt.imshow(img,cmap ='gray')\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c687d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe020f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
